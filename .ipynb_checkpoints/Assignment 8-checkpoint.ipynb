{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce7441c",
   "metadata": {},
   "source": [
    "<b>1. What exactly is a feature? Give an example to illustrate your point.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11f3cd",
   "metadata": {},
   "source": [
    "Ans: Computers are monolingual, they speak numbers.\n",
    "\n",
    "In order to use a machine learning algorithm, you have to turn your inputs into entities the algorithm can understand: numbers.\n",
    "\n",
    "For example, a computer doesn't know anything about an image. To the computer, an image is just a bunch of pixel values.\n",
    "\n",
    "With machine learning on images, often the pixel values are the features, but you could also do higher level feature engineering - for instance, you could measure the average pixel intensity, or you could try to detect the edges and provide a count of how many edges exist in the top quadrant of the image.\n",
    "\n",
    "The pic below is the number 5, in pixel form to give you an idea.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9dd40",
   "metadata": {},
   "source": [
    "<b>2. What are the various circumstances in which feature construction is required?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b27e99",
   "metadata": {},
   "source": [
    "Ans: 1.Imputation\n",
    "2.Handling Outliers\n",
    "3.Binning\n",
    "4.Log Transform\n",
    "5.One-Hot Encoding\n",
    "6.Grouping Operations\n",
    "7.Feature Split\n",
    "8.Scaling\n",
    "9.Extracting Date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ce37b0",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c03d6",
   "metadata": {},
   "source": [
    "<b>3. Describe how nominal variables are encoded.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb1855",
   "metadata": {},
   "source": [
    "Ans: In ordinal encoding, each unique category value is assigned an integer value.\n",
    "\n",
    "For example, “red” is 1, “green” is 2, and “blue” is 3.\n",
    "\n",
    "This is called an ordinal encoding or an integer encoding and is easily reversible. Often, integer values starting at zero are used.\n",
    "\n",
    "For some variables, an ordinal encoding may be enough. The integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship.\n",
    "\n",
    "It is a natural encoding for ordinal variables. For categorical variables, it imposes an ordinal relationship where no such relationship may exist. This can cause problems and a one-hot encoding may be used instead.\n",
    "\n",
    "This ordinal encoding transform is available in the scikit-learn Python machine learning library via the OrdinalEncoder class.\n",
    "\n",
    "By default, it will assign integers to labels in the order that is observed in the data. If a specific order is desired, it can be specified via the “categories” argument as a list with the rank order of all expected labels.\n",
    "\n",
    "We can demonstrate the usage of this class by converting colors categories “red”, “green” and “blue” into integers. First the categories are sorted then numbers are applied. For strings, this means the labels are sorted alphabetically and that blue=0, green=1 and red=2.\n",
    "\n",
    "The complete example is listed below.\n",
    "\n",
    "#example of a ordinal encoding\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "#define data\n",
    "data = asarray([['red'], ['green'], ['blue']])\n",
    "print(data)\n",
    "#define ordinal encoding\n",
    "encoder = OrdinalEncoder()\n",
    "#transform data\n",
    "result = encoder.fit_transform(data)\n",
    "print(result)\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "#example of a ordinal encoding\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "#define data\n",
    "data = asarray([['red'], ['green'], ['blue']])\n",
    "print(data)\n",
    "#define ordinal encoding\n",
    "encoder = OrdinalEncoder()\n",
    "#transform data\n",
    "result = encoder.fit_transform(data)\n",
    "print(result)\n",
    "Running the example first reports the 3 rows of label data, then the ordinal encoding.\n",
    "\n",
    "We can see that the numbers are assigned to the labels as we expected.\n",
    "\n",
    "[['red']\n",
    " ['green']\n",
    " ['blue']]\n",
    "[[2.]\n",
    " [1.]\n",
    " [0.]]\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "[['red']\n",
    " ['green']\n",
    " ['blue']]\n",
    "[[2.]\n",
    " [1.]\n",
    " [0.]]\n",
    "This OrdinalEncoder class is intended for input variables that are organized into rows and columns, e.g. a matrix.\n",
    "\n",
    "If a categorical target variable needs to be encoded for a classification predictive modeling problem, then the LabelEncoder class can be used. It does the same thing as the OrdinalEncoder, although it expects a one-dimensional input for the single target variable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce36130d",
   "metadata": {},
   "source": [
    "<b>4. Describe how numeric features are converted to categorical features.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e71da",
   "metadata": {},
   "source": [
    "Ans: \n",
    "CASE-1:- Giving numbers\n",
    "One way to deal with them is by assigning a number to them. Meaning, if I consider the above example of hair color then assigning black hair=1, grey hair=2, brown hair=3, and so on. Though this approach is easy to handle categorical variables, there is a problem with it. The problem is, by assigning a number to each of the types of hair we are creating an artificial order. In terms of numbers we can say that 2>1 or 3>2 but when we consider the actual meaning of the number that is the color of the hair, we are actually trying to say that grey hair > black hair and brown hair > grey hair. This is an absurd statement to make. So, by converting it to simple numbers, we are creating \n",
    "an artificial order which is not genuine, and to solve this problem we use another method called one-hot encoding.\n",
    "\n",
    "CASE-2:- One hot encoding\n",
    "This is a very interesting concept to handle categorical variables. What we do here is, say for hair color, we have 3 distinct values in my overall data set that is black, grey, and brown. What we do is, create 3 more features/columns with the headers black, grey, and brown respectively and assign the value 0 or 1 (indicating false or true). Suppose, say a person has black hair then we place 1 under the black hair column and the rest will be assigned 0. Similarly, if a person has grey hair then we assign 1 to the grey hair column, and the rest we assign it to 0. One hot encoding, basically, creates a binary vector of the size of the number of distinct elements (in our case 3).\n",
    " The biggest advantage of this method is that we get rid of the artificial ordering that we were facing in case-1\n",
    "\n",
    " CASE-3:- Mean Replacement technique\n",
    "Let us go back to the objective/problem we are trying to solve. Given a set of features, we need to determine the height of a person. What we do is, replace the name of the country with the average value of the height of a person belonging to that country. Meaning, suppose we want to replace “India” with a numeric value, so we calculate the average\n",
    " value of the height of people belonging from India (say we get that as 160cm) and then replace “India” with 160cm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd58e50",
   "metadata": {},
   "source": [
    "<b>\n",
    "5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7b9fc",
   "metadata": {},
   "source": [
    "Ans: Forward Selection: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "Backward Elimination: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "Recursive Feature elimination: It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted.\n",
    " It then ranks the features based on the order of their elimination.\n",
    "One of the best ways for implementing feature selection with wrapper methods is to use Boruta package that finds the importance of a feature by creating shadow features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f1c95",
   "metadata": {},
   "source": [
    "<b>6. When is a feature considered irrelevant? What can be said to quantify it?\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8507c79",
   "metadata": {},
   "source": [
    "Ans: Irrelevant or partially relevant features can negatively impact model performance.\n",
    "Feature selection and Data cleaning should be the first and most important step of your model designing.\n",
    "In this post, you will discover feature selection techniques that you can use in Machine Learning.\n",
    "Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2c73a",
   "metadata": {},
   "source": [
    "<b>\n",
    "7. When is a function considered redundant? What criteria are used to identify features that could be redundant?\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e6ea0",
   "metadata": {},
   "source": [
    "Ans: Sometimes data redundancy happens by accident while other times it is intentional. Accidental data redundancy can be the result of a complex process or inefficient coding while intentional data redundancy can be used to protect data and ensure consistency — simply by leveraging the multiple occurrences of data for disaster recovery and quality checks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93017191",
   "metadata": {},
   "source": [
    "<b>8. What are the various distance measurements used to determine feature similarity?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7855d",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Role of Distance Measures\n",
    "Hamming Distance\n",
    "Euclidean Distance\n",
    "Manhattan Distance (Taxicab or City Block)\n",
    "Minkowski Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e1c4c",
   "metadata": {},
   "source": [
    "<b>9. State difference between Euclidean and Manhattan distances?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73210b19",
   "metadata": {},
   "source": [
    "Ans: Euclidean distance is the shortest path between source and destination  but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines as shown in Figure 1.4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4ea21",
   "metadata": {},
   "source": [
    "<b>10. Distinguish between feature transformation and feature selection.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac5cd3",
   "metadata": {},
   "source": [
    "Ans: \n",
    "The key difference between feature selection and feature extraction techniques used for dimensionality reduction is that while the original features are maintained in the case of feature selection algorithms,\n",
    " the feature extraction algorithms transform the data onto a new feature space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d0336",
   "metadata": {},
   "source": [
    "<b>11. Make brief notes on any two of the following:\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea435f",
   "metadata": {},
   "source": [
    "1.SVD (Standard Variable Diameter Diameter)\n",
    "Perhaps the most known and widely used matrix decomposition method is the Singular-Value Decomposition, or SVD. All matrices have an SVD, which makes it more stable than other methods,\n",
    " such as the eigendecomposition. As such, it is often used in a wide array of applications including compressing, denoising, and data reduction.\n",
    "          2. Collection of features using a hybrid approach\n",
    "\n",
    "          3. The width of the silhouette\n",
    "          Silhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1.\n",
    "1: Means clusters are well apart from each other and clearly distinguished.\n",
    "0: Means clusters are indifferent, or we can say that the distance between clusters is not significant.\n",
    "-1: Means clusters are assigned in the wrong way.\n",
    "Silhouette Score = (b-a)/max(a,b)\n",
    "where\n",
    "a= average intra-cluster distance i.e the average distance between each point within a cluster.\n",
    "b= average inter-cluster distance i.e the average distance between all clusters.\n",
    "Calculating Silhouette Score\n",
    "Importing libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "%matplotlib inline\n",
    "\n",
    "          4. Receiver operating characteristic curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c88d918",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1b9f1b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "846331ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129bd51a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

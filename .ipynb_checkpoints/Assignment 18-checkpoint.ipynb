{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e55a39",
   "metadata": {},
   "source": [
    "<b>\n",
    "1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628182e1",
   "metadata": {},
   "source": [
    "Ans: \n",
    "The main distinction between the two approaches is the use of labeled datasets. To put it simply, supervised learning uses labeled input and output data, while an unsupervised learning algorithm does not.\n",
    "Unsupervised learning models, in contrast, work on their own to discover the inherent structure of unlabeled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f33521f",
   "metadata": {},
   "source": [
    "<b>2. Mention a few unsupervised learning applications.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12fcec7",
   "metadata": {},
   "source": [
    "Ans:\n",
    "The main applications of unsupervised learning include clustering, visualization, dimensionality reduction, finding association rules, and anomaly detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a56b4e7",
   "metadata": {},
   "source": [
    "<b>3. What are the three main types of clustering methods? Briefly describe the characteristics of each.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f1c66c",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Connectivity-based Clustering (Hierarchical clustering)\n",
    "Centroids-based Clustering (Partitioning methods)\n",
    "Distribution-based Clustering.\n",
    "Density-based Clustering (Model-based methods)\n",
    "Fuzzy Clustering.\n",
    "Constraint-based (Supervised Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483da7d7",
   "metadata": {},
   "source": [
    "<b>4. Explain how the k-means algorithm determines the consistency of clustering</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a20598",
   "metadata": {},
   "source": [
    "Ans:\n",
    "K centroids are created randomly (based on the predefined value of K)\n",
    "K-means allocates every data point in the dataset to the nearest centroid (minimizing Euclidean distances between them), meaning that a data point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid\n",
    "Then K-means recalculates the centroids by taking the mean of all data points assigned to that centroid’s cluster, hence reducing the total intra-cluster variance in relation to the previous step. The “means” in the K-means refers to averaging the data and finding the new centroid\n",
    "The algorithm iterates between steps 2 and 3 until some criteria is met (e.g. the sum of distances between the data points and their corresponding centroid is minimized, a maximum number of iterations is reached, no changes in centroids value or no data points change clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d97aaf4",
   "metadata": {},
   "source": [
    "<b>5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e9091",
   "metadata": {},
   "source": [
    "Ans: \n",
    "K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster. \n",
    "In contrast to the k -means algorithm, k -medoids chooses datapoints as centers ( medoids or exemplars)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f37341",
   "metadata": {},
   "source": [
    "<b>6. What is a dendrogram, and how does it work? Explain how to do it.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3828c3",
   "metadata": {},
   "source": [
    "Ans: \n",
    "A dendrogram is a diagram that shows the attribute distances between each pair of sequentially merged classes. To avoid crossing lines, the diagram is graphically arranged so that\n",
    " members of each pair of classes to be merged are neighbors in the diagram. The Dendrogram tool uses a hierarchical clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6896eeb8",
   "metadata": {},
   "source": [
    "<b>7. What exactly is SSE? What role does it play in the k-means algorithm?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca362aa6",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Clustering is a set of techniques used to partition data into groups, or clusters. Clusters are loosely defined as groups of data objects that are more similar to other objects in their cluster than they are to data objects in other clusters. In practice, clustering helps identify two qualities of data:\n",
    "\n",
    "Meaningfulness\n",
    "Usefulness\n",
    "Meaningful clusters expand domain knowledge. For example, in the medical field, researchers applied clustering to gene expression experiments. The clustering results identified groups of patients who respond differently to medical treatments.\n",
    "\n",
    "Useful clusters, on the other hand, serve as an intermediate step in a data pipeline. For example, businesses use clustering for customer segmentation. The clustering results segment customers into groups with similar purchase histories, which businesses can then use to create targeted advertising campaigns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97f900",
   "metadata": {},
   "source": [
    "<b>8. With a step-by-step algorithm, explain the k-means procedure.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da4a84",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "k-means is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed apriori. The main idea is to define k centers, one for each cluster.\n",
    "Let  X = {x1,x2,x3,……..,xn} be the set of data points and V = {v1,v2,…….,vc} be the set of centers.\n",
    "\n",
    "1) Randomly select ‘c’ cluster centers.\n",
    "\n",
    "2) Calculate the distance between each data point and cluster centers.\n",
    "\n",
    "3) Assign the data point to the cluster center whose distance from the cluster center is minimum of all the cluster centers..\n",
    "\n",
    "4) Recalculate the new cluster center using: \n",
    "\n",
    "where, ‘ci’ represents the number of data points in ith cluster.\n",
    "\n",
    "\n",
    "5) Recalculate the distance between each data point and new obtained cluster centers.\n",
    "\n",
    "6) If no data point was reassigned then stop, otherwise repeat from step 3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94fedc",
   "metadata": {},
   "source": [
    "<b>9. In the sense of hierarchical clustering, define the terms single link and complete link.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418b4b8",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Clustering is a solution to the problem of unsupervised machine learning.\n",
    "\n",
    "Single linkage and complete linkage are two algorithms of agglomerative Hierarchical clustering. In agglomerative hierarchical clustering we start with each data point as a single cluster and then iteratively join clusters into bigger clusters till we reach one single cluster with all the data points. This cluster hierarchy is often represented using a dendrogram. Depending on where we cut the tree determines how many clusters we have.\n",
    "\n",
    "So how do we decide which two clusters to merge at a certain level? Single linkage and complete linkage differ in this.\n",
    "\n",
    "In single linkage, the distance between two clusters is the minimum distance between the two groups, i.e. the distance between the data points closest to the other cluster is taken as the distance between the two clusters. On the other hand, in complete linkage the distance between the farthest points are taken as the intra cluster distance.\n",
    "\n",
    "For example, let the data points be on the  R.  Say the data points be 0, 3,10,11,19,20. First, we get 4 clusters - {0},{3},{10,11},{19,20}. Next {0} and {3} merge to form {0,3}. Upto here both single and complete linkage are congruent.\n",
    "\n",
    "In the next step, in single linkage the distance between {0,3} and {10,11} is 10-3 = 7 while distance between {10,11} and {19,20} is 19-11=8. Hence the next merge is {0, 3} and {10,11}.\n",
    "\n",
    "In complete linkage distance({0,3},{10, 11}) = 11, while distance({10, 11}, {19,20}) = 10. Thus we merge the latter pair.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad65383",
   "metadata": {},
   "source": [
    "<b>10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c50dbd",
   "metadata": {},
   "source": [
    "Ans:\n",
    "Apriori is the first association rule mining algorithm that pioneered the use\n",
    "of support-based pruning to systematically control the exponential growth of\n",
    "candidate itemsets. Figure 5.5 provides a high-level illustration of the frequent\n",
    "itemset generation part of the Apriori algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

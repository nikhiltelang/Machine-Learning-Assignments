{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ce93cb",
   "metadata": {},
   "source": [
    "<b>1. Is there any way to combine five different models that have all been trained on the same training data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is the reason?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae86340",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Final machine learning model is a model that you use to make predictions on new data.\n",
    "\n",
    "That is, given new examples of input data, you want to use the model to predict the expected output. This may be a classification (assign a label) or a regression (a real value).\n",
    "\n",
    "For example, whether the photo is a picture of a dog or a cat, or the estimated number of sales for tomorrow.\n",
    "\n",
    "The goal of your machine learning project is to arrive at a final model that performs the best, where “best” is defined by:\n",
    "\n",
    "Data: the historical data that you have available.\n",
    "Time: the time you have to spend on the project.\n",
    "Procedure: the data preparation steps, algorithm or algorithms, and the chosen algorithm configurations.\n",
    "In your project, you gather the data, spend the time you have, and discover the data preparation procedures, algorithm to use, and how to configure it.\n",
    "\n",
    "The final model is the pinnacle of this process, the end you seek in order to start actually making predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527f1ee",
   "metadata": {},
   "source": [
    "<b>2. What's the difference between hard voting classifiers and soft voting classifiers?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec372b91",
   "metadata": {},
   "source": [
    "Ans: \n",
    "In hard voting (also known as majority voting), every individual classifier votes for a class, and the majority wins. In statistical terms, the predicted target label of the ensemble is the mode of the distribution of individually predicted labels.\n",
    "In soft voting, every individual classifier provides a probability value that a specific data point belongs to a particular target class. The predictions are weighted by the classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419eda74",
   "metadata": {},
   "source": [
    "<b>3. Is it possible to distribute a bagging ensemble's training through several servers to speed up the process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all options.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e14b5",
   "metadata": {},
   "source": [
    "Ans: \n",
    " It is quite possible to speed up training of a bagging ensemble, pasting ensembles \n",
    " and Random Forests by distributing it across multiple servers, since each predictor in the ensemble is independent of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ffc99",
   "metadata": {},
   "source": [
    "<b>4. What is the advantage of evaluating out of the bag?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71fcfa",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging). Bagging uses subsampling with replacement to create training samples for the model to learn from. \n",
    "OOB error is the mean prediction error on each training sample xi, using only the trees that did not have xi in their bootstrap sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9802e",
   "metadata": {},
   "source": [
    "<b>5. What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random Forests?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc522ea",
   "metadata": {},
   "source": [
    "Ans: Random Forest chooses the optimum split while Extra Trees chooses it randomly. However, once the split points are selected, the two algorithms choose the best one between all the subset of features. \n",
    "Therefore, Extra Trees adds randomization but still has optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820661d",
   "metadata": {},
   "source": [
    "<b>6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training data?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddfb0c5",
   "metadata": {},
   "source": [
    "Ans: \n",
    "If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how? try increasing the number of estimators \n",
    "or reducing the regularization hyperparameters of the base estimator, also try slightly increasing the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b4dae",
   "metadata": {},
   "source": [
    "<b>7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the training set?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df93589b",
   "metadata": {},
   "source": [
    "Ans: \n",
    "decreasing the learning rate, early stopping to find the right number of predictors (you probably have too many)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

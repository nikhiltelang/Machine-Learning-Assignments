{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f359c3",
   "metadata": {},
   "source": [
    "<b>1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead669a",
   "metadata": {},
   "source": [
    "Ans: \n",
    "PCA is a special case of factor analysis where factors are found using spectral decomposition of Eigen values. There are other rotations (later in the answer) that use factor analysis and not PCA.\n",
    "But there are some inherent problems with PCA (tying back to your question here) that have caused a reduction in its usage. PCA or principal component analysis has three properties viz. maximizing the variance, orthogonality of basis and dimension reduction.\n",
    "\n",
    "But due to the first point i.e. maximization of variance in the p-dimensional space of the data under a quadratic constraint, leads to a large number of variables loading in on the first principal component. Although this leads to effective dimension reduction, there is little more you can do with these PCs as the interpretation is lost.\n",
    "\n",
    "For e.g. If you have 10 variables, variance in 8 or more can typically be explained by 1 or 2 principal components. Now how would you adjust this variable in real life? You cant since you dont have the original variable anymore but only principal components.\n",
    "\n",
    "This is a major drawback that your question brings to light. A way to avoid this is to use Varimax or Orthomax rotation such that although the R squares, orthogonality and dimension reduction are similar, components do not align in the direction of maximum variance. This might be desirable as you will now have 2 or 3 variables aligning with a component (which is no longer principal component)\n",
    "\n",
    "Secondly, although many research papers do this, it makes absolutely no sense to use PCA on categorical variables. In practice, one will seldom have only numeric variables.\n",
    "\n",
    "Thirdly, one cannot extract the original variables or act on these levers as I mentioned in the example above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4832a0e8",
   "metadata": {},
   "source": [
    "<b>2. What is the dimensionality curse?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa2aa3",
   "metadata": {},
   "source": [
    "Ans: \n",
    "The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b63ef",
   "metadata": {},
   "source": [
    "<b>3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973b139",
   "metadata": {},
   "source": [
    "Ans: \n",
    "But this alone will not be sufficient if you want to reduce the dimensions as it's highly subjective and you need to tune the variance threshold manually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f5cb62",
   "metadata": {},
   "source": [
    "<b>4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e01818",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Principal components analysis can be used to reduce the dimensionality of any quantitative data that can be put into a two-dimensional array. But you don’t need PCA, you can reduce the dimensionality by any number of methods, including just chopping columns out of your array.\n",
    "\n",
    "So I assume you want to know if the dimensionality reduction from PCA will be useful or not.\n",
    "\n",
    "When you say a dataset is “nonlinear” you might mean that the pairwise scatterplots of variables show patterns that are not straight lines, or that the data are generated by a complex or unstable process. Neither the shapes suggested by pairwise scatterplots nor the underlying system that generated the data influence whether or not PCA will give a useful dimensionality reduction.\n",
    "\n",
    "If your dataset has outliers, then they can dominate the PCA calculation. If the outliers are not of great interest—for example if they are data errors or unusual cases and you want to study the common cases—then PCA may not do what you want. Nonlinear systems can produce outliers, but they don’t always do that, and you don’t need nonlinearity for outliers.\n",
    "\n",
    "If your dataset has clusters, and these are your main interest, PCA may not be useful. But this too is unrelated to linearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715510b",
   "metadata": {},
   "source": [
    "<b>5. Assume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd103fc",
   "metadata": {},
   "source": [
    "Ans: \n",
    "If I perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%.  In this case roughly 950 dimensions are required to preserve 95% of the variance. So the answer is, it depends on the dataset, and it could be any number between 1 and 950.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78157986",
   "metadata": {},
   "source": [
    "<b>6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18511840",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Vanilla PCA: the dataset fit in memory\n",
    "Incremental PCA: larget dataset that don't fit in memory, online taks\n",
    "Randomized PCA: considerably reduce dimensionality and the dataset fit the memory.\n",
    "kenrl PCA: used for nonlinear PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f69df2",
   "metadata": {},
   "source": [
    "<b>7. How do you assess a dimensionality reduction algorithm's success on your dataset?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6547cf7",
   "metadata": {},
   "source": [
    "Ans: \n",
    " Dimensionality reduction seeks a lower-dimensional representation of numerical input data that preserves the salient relationships in the da\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b18ca",
   "metadata": {},
   "source": [
    "<b>8. Is it logical to use two different dimensionality reduction algorithms in a chain?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6166ce2",
   "metadata": {},
   "source": [
    "Ans:\n",
    "Indeed, it often make any sense to chain two different dimensionality reduction algorithms. A common example is using PCA to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
